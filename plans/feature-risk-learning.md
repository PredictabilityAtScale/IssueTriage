# Feature Plan: Historical Risk Learning

## Goal
Enable IssueTriage to forecast risk metrics for new issues or pull requests by learning from historical repository activity and feeding those insights into the assessment workflow.

## Target Outcomes
- Predict likelihood of high, medium, or low risk for a new triage item before linked work exists.
- Generate estimated drivers (e.g., expected change volume, review friction) to seed readiness guidance.
- Surface similar historical issues/PRs to provide reviewers with evidence for the prediction.

## Similarity Discovery Strategy
- Capture per-issue similarity snapshots when issues close by persisting:
   - Final label set, assignees, milestone, linked PR metrics, and risk outcomes.
   - Generated "likeness summaries" that describe components touched, change scope, and notable risk drivers.
- Compute semantic embeddings (sentence-transformer or Azure OpenAI) for title, body, and likeness summary; store vectors alongside structured fingerprints (e.g., label hashes, hot-file counts).
- Maintain an incremental similarity index (Faiss, Pinecone, Redis, or in-process ANN) keyed by issue id with timestamps for temporal filtering.
- Serve a `SimilarityService` that blends cosine similarity with structured overlap metrics (label Jaccard, shared files) to rank top matches.
- Surface retrieved matches inside assessments, webview UI, and as prompt context for downstream LLM/ML predictions.

## Data Requirements
- **Source repositories**: choose one or more public repos with rich history (e.g., `microsoft/vscode`, `numpy/numpy`).
- **Data types**:
  - Issue metadata: titles, bodies, labels, milestones, timestamps, author.
  - Linked PR data: churn metrics, review state counts, merge outcomes, time-to-merge.
  - Post-merge signals: subsequent bug reports, reverts, security advisories (optional but valuable).
  - Assessment + risk records generated by IssueTriage once instrumentation exists.
- Similarity captures: likeness summaries, embedding vectors, structured fingerprints saved when issues close or PRs merge.
- **Collection tooling**: extend `GitHubClient` or create scripts to export historical snapshots, respecting rate limits and caching.
- **Storage**: structured dataset (Parquet/SQLite/Postgres) with normalized tables for issues, PRs, risk outcomes, derived features, plus a similarity index table storing vectors and metadata for retrieval.

## Feature Engineering
- Text embeddings from issue titles/bodies (OpenAI/Azure, sentence-transformers) for similarity and semantic features.
- Numeric aggregates: historical churn averages per label/component, team cadence, time-based trend features.
- Graph features: author collaboration networks, file hot-spots, PR dependency counts.
- Labels for supervised learning: existing `riskLevel`, `riskScore`, binary outcomes (e.g., high-risk vs non-high-risk).

## Modeling Strategy
1. **Baseline Heuristics**
   - Rule-based scoring using historical averages per label/component.
   - Serves as benchmark and fallback.
2. **Classical ML**
   - Train gradient boosting or random forest classifiers/regressors on engineered features.
   - Predict risk level (classification) and risk score (regression).
   - Evaluate with cross-validation, precision/recall for high-risk, MAE for scores.
3. **LLM-Assisted**
   - Prompt LLM with structured historical context plus new issue details to obtain predicted metrics.
   - Optionally fine-tune small models (LLaMA-family) with instruction data generated from historical pairs.
   - Compare performance vs ML models; consider ensemble (LLM justification + ML probability).

## Evaluation Plan
- Split dataset by time (train on past periods, test on future intervals) to mimic real deployment.
- Metrics:
  - Classification: F1, recall on high-risk, ROC-AUC.
  - Regression: MAE / RMSE on riskScore.
  - Similarity retrieval: precision@k for finding truly related past issues.
- Baseline comparison: ensure model outperforms simple heuristics.
- Statistical significance tests when combining multiple repos.

## Integration Steps
1. **Data Pipeline**: batch exporter to populate historical dataset; schedule periodic refresh.
2. **Model Serving**: package trained model (e.g., ONNX or JSON weights) accessible from VS Code extension via local service or remote API.
3. **Extension Wiring**:
   - Add `PredictedRiskService` to request predictions when issue loads.
   - Merge outputs with existing `RiskIntelligenceService` summaries.
   - Display confidence, key drivers, and similar past items in the IssueTriage panel.
   - Integrate `SimilarityService` to return top-k matches from precomputed embeddings/fingerprints for UI and prompt augmentation.
4. **Prompt Augmentation**: feed predicted metrics and retrieved historical snippets into the assessment payload for OpenRouter.

## Testing Strategy
- Offline unit tests for feature extractors and data loaders.
- Regression tests validating model predictions against a frozen evaluation set.
- Integration tests simulating extension calls against a mock service.
- Manual validation using selected public repo issues to check qualitative usefulness.

## Tooling & Infrastructure
- Python or TypeScript data pipeline (consider `scripts/` folder) leveraging GitHub REST/GraphQL.
- ML stack: scikit-learn/lightGBM for classical models, Hugging Face transformers or Azure OpenAI for embeddings/LLM prompts.
- Experiment tracking (MLflow, Weights & Biases) to log runs and metrics.
- Versioned model artifacts stored in storage bucket or repository releases.

## Risks & Mitigations
- **Sparse labeled data**: bootstrap with heuristic labels, progressively replace with real assessment outcomes.
- **API limits**: implement caching/backoff, consider GitHub Archival datasets (GH Archive, BigQuery).
- **Model drift**: schedule retraining; monitor telemetry from live predictions.
- **Privacy**: ensure no sensitive repository data stored without consent; default to public repos for initial models.

## Timeline (High-Level)
1. **Weeks 1-2**: Data audit, repository selection, pipeline scaffold.
2. **Weeks 3-5**: Feature engineering, baseline heuristics, initial ML model.
3. **Weeks 6-8**: LLM prompt experiments, similarity retrieval integration.
4. **Weeks 9-10**: Extension integration prototype, UI surfacing, offline evaluation suite.
5. **Week 11+**: Feedback loop, telemetry instrumentation, incremental deployment.

## Next Actions
- Confirm pilot repository list and secure API tokens.
- Define schema for historical dataset and implement initial export script.
- Choose experiment tracking approach and spin up environment (local or cloud notebook).
- Draft specification for `PredictedRiskService` interface inside the extension.
- Design likeness summary generator and similarity index schema; prototype offline ANN lookup against exported data.
